
To Do List / Change Log

0.1 (09 Feb 2014) - Initial Commit
  - Show cluster summary state - nodes, volumes and brick up/down state
  - show summary information per volume - brick up/down count, volume type, 
    volume status and usable capacity summary
  - show errors discovered in summary list at the end of the output

0.2 Complete 2014/02/24
  - align volume state text with nagios monitoring project (DONE)
  - add self heal status (DONE)

0.3 Complete 2014/03/01
  - add check for self heal backlog and show it in volume summary info (DONE)
  - add protocol summary to volume info (DONE)
  - reformatted the summary display to make it more readable/les cluttered (DONE)
  - fix - in vol status sometimes the node name from gluster is not resolved, 
    so pick this up and resolve 'manually' (DONE)

0.4 Complete 2014/03/04
  - add a version check, to abort the run if gluster is not >= 3.4 (DONE)
  - add capacity info to the summary section (DONE)
  - add pct used for each volume (DONE)
  - add volume detail information (DONE)
    - vol components, replica set relationships
  - fix - malformed xml from gluster showing bricks against wrong vols

0.45 Complete 2014/03/06
  - fix release
    - node discovery now validated against initial brick list from a vol info
      avoiding name issues (hopefully!)
    - gstatus detects whether the cluster is name or ip based, and changes 
      name resolution logic accordingly
    - merged volume methods for calculating self heal states
    - added code to check gluster command o/p return code (on vol status) since
      --xml does not set shell return code ($?)
  - New additions
    - added a message count against the cluster state, to immediately show how
      many errors a cluster may have i.e. UNHEALTHY(5) --> 5 errors found

0.5 Complete 2014/03/21
  - add output options for json, keyvalue and console mode (console is default ie. stdout)
  - add client connection counts (glusterfs connections)
  
0.56
  - added debug mode (-D) 
  - added a config module for global vars across the modules
  - updated hostname resolution to use shortname and FQDN
  - add code in main gluster module  for debug.  
  
0.59 August 2014
  - refactor the node definition logic (now based on uuid not name)
  - fix - supports cluster defined on fqdn and volumes using shortnames and IP based clusters
  - fix - corrected cluster capacity calculations when brick(s) used by multiple volumes
  - added capacity overcommit flag if the same device is referenced by multiple bricks
  - added overcommit status field added to output (console,keyvalue,json)
  - added volume stats to json output mode  
  - added info message when bricks are missing to confirm the capacity info is inaccurate
  - added -n option to skip self heal info gathering
  - misc doc updates and refresh of examples directory
  

  
0.6 planned
  - add task info to the volume state to show when rebalance is active
  - provide threshold triggers
    - space usage triggers on volumes 
    - space usage trigger on bricks

0.7 planned
  - address the blocking problem when a node goes down (commands to a separate thread with a 
    timeout?) 

0.8 planned
  - add brick check to make recommendations on mount point definitions/filesystem, and an option to suppress recommendations

Items being considered
- RFE - Peter AuYeung - for volume capacity, include quota information if enabled
- RFE - Peter AuYeung - for the xml output provide a volume option


Known BUGS
- invocation immediately after a node down event will block vol status. This needs to be 
  detected and the run aborted to prevent the wrong information being shown to
  the user
- gluster is not returning valid xml when nodes are not present in the config. The problem gets worse with cluster size, and disruption. Current testing has nly allowed 4 node tests to be performed. WORKED AROUND 0.4 onwards
- when multiple replicated volumes are present, the count of self heal processes is incorrect


